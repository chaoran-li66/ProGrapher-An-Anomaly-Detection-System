{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd1f13d-318b-4f8d-9dc8-c88667791c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.7.0+cpu\n",
      "DGL版本: 2.0.0\n",
      "文件是否存在: True\n",
      "文件大小: 2.13 GB\n",
      "初始内存使用: 15.04 GB\n",
      "可用CPU核数: 22\n",
      "开始时间: 2025-06-29 20:21:14\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import dgl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import logging\n",
    "import time\n",
    "import gc\n",
    "import psutil\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(filename='process.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# 数据集路径\n",
    "DATA_PATH = r\".\\Data\\StreamSpot\\all.tsv\"\n",
    "OUTPUT_DIR = r\".\\Data\\StreamSpot\\processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 检查环境\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"DGL版本: {dgl.__version__}\")\n",
    "print(f\"文件是否存在: {os.path.exists(DATA_PATH)}\")\n",
    "print(f\"文件大小: {os.path.getsize(DATA_PATH) / 1024**3:.2f} GB\")\n",
    "print(f\"初始内存使用: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "print(f\"可用CPU核数: {os.cpu_count()}\")\n",
    "logger.info(\"环境检查完成\")\n",
    "start_time = time.time()\n",
    "print(f\"开始时间: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\")\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e003507c-0106-4568-aef0-5bdf3a28289b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行预处理...\n",
      "开始预处理...\n",
      "处理chunk 0，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 1，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 2，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 3，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 4，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 5，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 6，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 7，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 8，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 9，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 10，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 11，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 12，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 13，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 14，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 15，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 16，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 17，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 18，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 19，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 20，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 21，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 22，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 23，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 24，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 25，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 26，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 27，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 28，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 29，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 30，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 31，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 32，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 33，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 34，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 35，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 36，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 37，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 38，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 39，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 40，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 41，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 42，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 43，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 44，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 45，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 46，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 47，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 48，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 49，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 50，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 51，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 52，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 53，行数: 500000，耗时: 0.35秒\n",
      "处理chunk 54，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 55，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 56，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 57，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 58，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 59，行数: 500000，耗时: 0.18秒\n",
      "处理chunk 60，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 61，行数: 500000，耗时: 0.18秒\n",
      "处理chunk 62，行数: 500000，耗时: 0.18秒\n",
      "处理chunk 63，行数: 500000，耗时: 0.18秒\n",
      "处理chunk 64，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 65，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 66，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 67，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 68，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 69，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 70，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 71，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 72，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 73，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 74，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 75，行数: 500000，耗时: 0.29秒\n",
      "处理chunk 76，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 77，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 78，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 79，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 80，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 81，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 82，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 83，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 84，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 85，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 86，行数: 500000，耗时: 0.33秒\n",
      "处理chunk 87，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 88，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 89，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 90，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 91，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 92，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 93，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 94，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 95，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 96，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 97，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 98，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 99，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 100，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 101，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 102，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 103，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 104，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 105，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 106，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 107，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 108，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 109，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 110，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 111，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 112，行数: 500000，耗时: 0.30秒\n",
      "处理chunk 113，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 114，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 115，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 116，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 117，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 118，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 119，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 120，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 121，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 122，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 123，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 124，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 125，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 126，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 127，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 128，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 129，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 130，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 131，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 132，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 133，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 134，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 135，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 136，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 137，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 138，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 139，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 140，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 141，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 142，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 143，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 144，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 145，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 146，行数: 500000，耗时: 0.28秒\n",
      "处理chunk 147，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 148，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 149，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 150，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 151，行数: 500000，耗时: 0.30秒\n",
      "处理chunk 152，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 153，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 154，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 155，行数: 500000，耗时: 0.27秒\n",
      "处理chunk 156，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 157，行数: 500000，耗时: 0.21秒\n",
      "处理chunk 158，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 159，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 160，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 161，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 162，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 163，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 164，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 165，行数: 500000，耗时: 0.19秒\n",
      "处理chunk 166，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 167，行数: 500000，耗时: 0.24秒\n",
      "处理chunk 168，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 169，行数: 500000，耗时: 0.22秒\n",
      "处理chunk 170，行数: 500000，耗时: 0.29秒\n",
      "处理chunk 171，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 172，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 173，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 174，行数: 500000，耗时: 0.25秒\n",
      "处理chunk 175，行数: 500000，耗时: 0.20秒\n",
      "处理chunk 176，行数: 500000，耗时: 0.23秒\n",
      "处理chunk 177，行数: 500000，耗时: 0.26秒\n",
      "处理chunk 178，行数: 500000，耗时: 0.47秒\n",
      "处理chunk 179，行数: 270902，耗时: 0.12秒\n",
      "节点数: 5044482, 节点类型数: 8, 边类型数: 26\n",
      "预处理总耗时: 200.66秒\n",
      "内存使用: 15.35 GB\n"
     ]
    }
   ],
   "source": [
    "# 加载和预处理StreamSpot数据集\n",
    "def preprocess_data_chunks(data_path, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    分块加载TSV文件，编码节点、节点类型和边类型\n",
    "    参数：\n",
    "        data_path: TSV文件路径\n",
    "        chunk_size: 每次读取的行数\n",
    "    返回：\n",
    "        节点编码器、节点类型编码器、边类型编码器\n",
    "    \"\"\"\n",
    "    start_preprocess = time.time()\n",
    "    logger.info(\"开始预处理\")\n",
    "    print(\"开始预处理...\")\n",
    "    \n",
    "    node_encoder = LabelEncoder()\n",
    "    node_type_encoder = LabelEncoder()\n",
    "    edge_encoder = LabelEncoder()\n",
    "    \n",
    "    nodes = set()\n",
    "    node_types = set()\n",
    "    edge_types = set()\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(pd.read_csv(data_path, sep='\\t', header=None,\n",
    "                                                 names=[\"source_id\", \"source_type\", \"dest_id\", \n",
    "                                                        \"dest_type\", \"edge_type\", \"graph_id\"],\n",
    "                                                 dtype={\"source_id\": str, \"dest_id\": str, \"graph_id\": int},\n",
    "                                                 chunksize=chunk_size)):\n",
    "        chunk_start = time.time()\n",
    "        nodes.update(chunk[\"source_id\"].astype(str))\n",
    "        nodes.update(chunk[\"dest_id\"].astype(str))\n",
    "        node_types.update(chunk[\"source_type\"])\n",
    "        node_types.update(chunk[\"dest_type\"])\n",
    "        edge_types.update(chunk[\"edge_type\"])\n",
    "        print(f\"处理chunk {chunk_idx}，行数: {len(chunk)}，耗时: {time.time() - chunk_start:.2f}秒\")\n",
    "        logger.info(f\"处理chunk {chunk_idx}，行数: {len(chunk)}，内存: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "        gc.collect()\n",
    "    \n",
    "    node_encoder.fit(list(nodes))\n",
    "    node_type_encoder.fit(list(node_types))\n",
    "    edge_encoder.fit(list(edge_types))\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, \"node_encoder.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(node_encoder, f)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"node_type_encoder.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(node_type_encoder, f)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"edge_encoder.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(edge_encoder, f)\n",
    "    \n",
    "    print(f\"节点数: {len(nodes)}, 节点类型数: {len(node_types)}, 边类型数: {len(edge_types)}\")\n",
    "    print(f\"预处理总耗时: {time.time() - start_preprocess:.2f}秒\")\n",
    "    print(f\"内存使用: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "    logger.info(f\"预处理完成，节点数: {len(nodes)}，耗时: {time.time() - start_preprocess:.2f}秒\")\n",
    "    return node_encoder, node_type_encoder, edge_encoder\n",
    "\n",
    "# 执行预处理\n",
    "print(\"执行预处理...\")\n",
    "node_encoder, node_type_encoder, edge_encoder = preprocess_data_chunks(DATA_PATH)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6607b3f7-62d5-4860-81e9-d25d3a362862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建DGL图\n",
    "def build_dgl_graph(src_ids, dst_ids, edge_types, node_types):\n",
    "    \"\"\"\n",
    "    从边列表和节点类型构建DGL图\n",
    "    参数：\n",
    "        src_ids: 源节点ID列表\n",
    "        dst_ids: 目标节点ID列表\n",
    "        edge_types: 边类型列表\n",
    "        node_types: 节点ID到节点类型的字典\n",
    "    返回：\n",
    "        DGL图\n",
    "    \"\"\"\n",
    "    start_build = time.time()\n",
    "    \n",
    "    # 重新映射节点ID为连续整数\n",
    "    unique_nodes = set(src_ids) | set(dst_ids)\n",
    "    node_map = {nid: i for i, nid in enumerate(unique_nodes)}\n",
    "    src_ids_mapped = [node_map[nid] for nid in src_ids]\n",
    "    dst_ids_mapped = [node_map[nid] for nid in dst_ids]\n",
    "    \n",
    "    g = dgl.graph((torch.tensor(src_ids_mapped, dtype=torch.int64), \n",
    "                   torch.tensor(dst_ids_mapped, dtype=torch.int64)))\n",
    "    g.edata[\"type\"] = torch.tensor(edge_types, dtype=torch.int64)\n",
    "    \n",
    "    # 设置节点类型\n",
    "    node_type_list = [node_types.get(nid, 0) for nid in unique_nodes]\n",
    "    g.ndata[\"type\"] = torch.tensor(node_type_list, dtype=torch.int64)\n",
    "    g.ndata[\"id\"] = torch.tensor(list(unique_nodes), dtype=torch.int64)\n",
    "    \n",
    "    print(f\"构建图，节点数: {len(unique_nodes)}，边数: {len(src_ids)}，耗时: {time.time() - start_build:.2f}秒\")\n",
    "    logger.info(f\"构建图，节点数: {len(unique_nodes)}，耗时: {time.time() - start_build:.2f}秒\")\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b281849-3dce-4090-9ed2-b5b38047beef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 01:04:26,849 - 生成 16625 个快照文件: ['snapshot_0.pt', 'snapshot_1.pt', 'snapshot_10.pt', 'snapshot_100.pt', 'snapshot_1000.pt']\n",
      "2025-06-30 01:04:26,849 - 生成 16625 个快照文件: ['snapshot_0.pt', 'snapshot_1.pt', 'snapshot_10.pt', 'snapshot_100.pt', 'snapshot_1000.pt']\n",
      "2025-06-30 01:04:26,850 - 生成 16625 个快照文件: ['snapshot_0.pt', 'snapshot_1.pt', 'snapshot_10.pt', 'snapshot_100.pt', 'snapshot_1000.pt']\n",
      "2025-06-30 01:04:26,850 - 生成 16625 个快照文件: ['snapshot_0.pt', 'snapshot_1.pt', 'snapshot_10.pt', 'snapshot_100.pt', 'snapshot_1000.pt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总时间: 12893.26秒, 内存: 16.38 GB\n",
      "生成 16625 个快照文件: ['snapshot_0.pt', 'snapshot_1.pt', 'snapshot_10.pt', 'snapshot_100.pt', 'snapshot_1000.pt']\n"
     ]
    }
   ],
   "source": [
    "# 优化后的快照生成\n",
    "import time\n",
    "import gc\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import psutil\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "# 配置全局开始时间（需与 Cell 1 一致）\n",
    "try:\n",
    "    start_time\n",
    "except NameError:\n",
    "    start_time = time.time()\n",
    "\n",
    "# 配置日志到 OUTPUT_DIR、备用日志和控制台\n",
    "OUTPUT_DIR = r\".\\Data\\StreamSpot\\processed\"\n",
    "log_path = os.path.abspath(os.path.join(OUTPUT_DIR, \"process.log\"))\n",
    "fallback_log_path = os.path.abspath(\"fallback_process.log\")\n",
    "try:\n",
    "    logging.basicConfig(filename=log_path, level=logging.INFO, format='%(asctime)s - %(message)s', filemode='w')\n",
    "    logger = logging.getLogger()\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.info(\"主日志配置成功\")\n",
    "    print(f\"主日志配置成功: {log_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"主日志配置失败: {e}\")\n",
    "    try:\n",
    "        logging.basicConfig(filename=fallback_log_path, level=logging.INFO, format='%(asctime)s - %(message)s', filemode='w')\n",
    "        logger = logging.getLogger()\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        logger.addHandler(console_handler)\n",
    "        logger.info(\"备用日志配置成功\")\n",
    "        print(f\"备用日志配置成功: {fallback_log_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"备用日志配置失败: {e}\")\n",
    "        logger = logging.getLogger()\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "        logger.addHandler(console_handler)\n",
    "        logger.info(\"仅控制台日志\")\n",
    "\n",
    "def generate_snapshots_chunks(data_path, node_encoder, node_type_encoder, edge_encoder, \n",
    "                             n=300, fr=1/3, chunk_size=500000, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    分块生成时间序列快照，保存为DGL图，处理所有边数 <= 50000 的图\n",
    "    参数：\n",
    "        data_path: TSV文件路径\n",
    "        node_encoder: 节点编码器\n",
    "        node_type_encoder: 节点类型编码器\n",
    "        edge_encoder: 边类型编码器\n",
    "        n: 快照大小\n",
    "        fr: 遗忘率\n",
    "        chunk_size: 每次读取的行数\n",
    "        output_dir: 输出目录\n",
    "    返回：\n",
    "        生成的快照数量\n",
    "    \"\"\"\n",
    "    start_snapshots = time.time()\n",
    "    logger.info(\"开始生成快照\")\n",
    "    print(\"开始生成快照...\")\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    cache_graph = defaultdict(list)\n",
    "    node_timestamps = {}\n",
    "    node_types = {}\n",
    "    node_count = 0\n",
    "    snapshot_id = 0\n",
    "    edge_batch_count = 0\n",
    "    \n",
    "    def print_status(message):\n",
    "        \"\"\"刷新式打印状态\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        print(f\"总时间: {time.time() - start_snapshots:.2f}秒, 内存: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "        print(message)\n",
    "        logger.info(message)\n",
    "    \n",
    "    def save_snapshot(g, sid):\n",
    "        \"\"\"保存快照到文件\"\"\"\n",
    "        start_save = time.time()\n",
    "        try:\n",
    "            if not os.access(output_dir, os.W_OK):\n",
    "                raise PermissionError(f\"无写入权限: {output_dir}\")\n",
    "            if psutil.disk_usage(output_dir).free < 1024**3:\n",
    "                raise RuntimeError(f\"磁盘空间不足: {output_dir}\")\n",
    "            snapshot_path = os.path.join(output_dir, f\"snapshot_{sid}.pt\")\n",
    "            torch.save(g, snapshot_path)\n",
    "            if not os.path.exists(snapshot_path):\n",
    "                raise RuntimeError(f\"文件未生成: {snapshot_path}\")\n",
    "            print_status(f\"保存快照 {sid}, 耗时: {time.time() - start_save:.2f}秒, 文件: {snapshot_path}\")\n",
    "            logger.info(f\"保存快照 {sid}, 耗时: {time.time() - start_save:.2f}秒, 文件: {snapshot_path}\")\n",
    "        except Exception as e:\n",
    "            print_status(f\"保存快照 {sid} 失败: {e}\")\n",
    "            logger.error(f\"保存快照 {sid} 失败: {e}\")\n",
    "    \n",
    "    def generate_snapshot():\n",
    "        \"\"\"从缓存图生成快照\"\"\"\n",
    "        nonlocal snapshot_id\n",
    "        if not cache_graph:\n",
    "            print_status(f\"尝试生成快照 {snapshot_id}, 但cache_graph为空\")\n",
    "            logger.warning(f\"尝试生成快照 {snapshot_id}, 但cache_graph为空\")\n",
    "            return False\n",
    "        src_ids, dst_ids, edge_types = [], [], []\n",
    "        for (s, d), etypes in cache_graph.items():\n",
    "            src_ids.append(s)\n",
    "            dst_ids.append(d)\n",
    "            edge_types.append(max(etypes))\n",
    "        print_status(f\"生成快照 {snapshot_id}, cache_graph大小: {len(cache_graph)}, 节点数: {node_count}, 边数: {len(src_ids)}\")\n",
    "        logger.info(f\"生成快照 {snapshot_id}, cache_graph大小: {len(cache_graph)}, 节点数: {node_count}, 边数: {len(src_ids)}\")\n",
    "        try:\n",
    "            g = build_dgl_graph(src_ids, dst_ids, edge_types, node_types)\n",
    "            print_status(f\"生成快照 {snapshot_id}, DGL图: 节点数={g.num_nodes()}, 边数={g.num_edges()}\")\n",
    "            logger.info(f\"生成快照 {snapshot_id}, DGL图: 节点数={g.num_nodes()}, 边数={g.num_edges()}\")\n",
    "            save_snapshot(g, snapshot_id)\n",
    "            snapshot_id += 1\n",
    "            cache_graph.clear()  # 清空cache_graph\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print_status(f\"生成快照 {snapshot_id} 失败: {e}\")\n",
    "            logger.error(f\"生成快照 {snapshot_id} 失败: {e}\")\n",
    "            return False\n",
    "    \n",
    "    processed_graph_ids = set()\n",
    "    for chunk_idx, chunk in enumerate(pd.read_csv(data_path, sep='\\t', header=None,\n",
    "                                                 names=[\"source_id\", \"source_type\", \"dest_id\", \n",
    "                                                        \"dest_type\", \"edge_type\", \"graph_id\"],\n",
    "                                                 dtype={\"source_id\": str, \"dest_id\": str, \"graph_id\": int},\n",
    "                                                 chunksize=chunk_size)):\n",
    "        chunk_start = time.time()\n",
    "        print_status(f\"处理chunk {chunk_idx}...\")\n",
    "        logger.info(f\"处理chunk {chunk_idx}\")\n",
    "        \n",
    "        # 预编码\n",
    "        try:\n",
    "            chunk[\"source_id_enc\"] = node_encoder.transform(chunk[\"source_id\"].astype(str))\n",
    "            chunk[\"dest_id_enc\"] = node_encoder.transform(chunk[\"dest_id\"].astype(str))\n",
    "            chunk[\"source_type_enc\"] = node_type_encoder.transform(chunk[\"source_type\"])\n",
    "            chunk[\"dest_type_enc\"] = node_type_encoder.transform(chunk[\"dest_type\"])\n",
    "            chunk[\"edge_type_enc\"] = edge_encoder.transform(chunk[\"edge_type\"])\n",
    "            print_status(f\"chunk {chunk_idx} 编码完成，行数: {len(chunk)}, 样本: {chunk[['source_id_enc', 'dest_id_enc', 'edge_type_enc']].head(1).to_string()}\")\n",
    "            logger.info(f\"chunk {chunk_idx} 编码完成，行数: {len(chunk)}, 样本: {chunk[['source_id_enc', 'dest_id_enc', 'edge_type_enc']].head(1).to_string()}\")\n",
    "        except Exception as e:\n",
    "            print_status(f\"chunk {chunk_idx} 编码失败: {e}\")\n",
    "            logger.error(f\"chunk {chunk_idx} 编码失败: {e}\")\n",
    "            continue\n",
    "        \n",
    "        graph_id_counts = chunk[\"graph_id\"].value_counts()\n",
    "        valid_graph_ids = [gid for gid in graph_id_counts.index if graph_id_counts[gid] <= 50000 and gid not in processed_graph_ids]\n",
    "        print_status(f\"chunk {chunk_idx} 有效graph_id: {valid_graph_ids}\")\n",
    "        logger.info(f\"chunk {chunk_idx} 有效graph_id: {valid_graph_ids}\")\n",
    "        \n",
    "        for graph_id in valid_graph_ids:\n",
    "            processed_graph_ids.add(graph_id)\n",
    "            edge_count = graph_id_counts[graph_id]\n",
    "            print_status(f\"开始处理graph_id: {graph_id}, 边数: {edge_count}\")\n",
    "            logger.info(f\"开始处理graph_id: {graph_id}, 边数: {edge_count}\")\n",
    "            \n",
    "            sub_df = chunk[chunk[\"graph_id\"] == graph_id]\n",
    "            try:\n",
    "                edge_start = time.time()\n",
    "                for idx in range(len(sub_df)):\n",
    "                    if idx % 100 == 0:\n",
    "                        print_status(f\"处理graph_id {graph_id}，进度: {idx}/{len(sub_df)}，节点数: {node_count}, cache_graph大小: {len(cache_graph)}\")\n",
    "                        logger.info(f\"graph_id {graph_id}，进度: {idx}/{len(sub_df)}，节点数: {node_count}, cache_graph大小: {len(cache_graph)}\")\n",
    "                    \n",
    "                    row = sub_df.iloc[idx]\n",
    "                    src = row[\"source_id_enc\"]\n",
    "                    dst = row[\"dest_id_enc\"]\n",
    "                    edge_type = row[\"edge_type_enc\"]\n",
    "                    src_type = row[\"source_type_enc\"]\n",
    "                    dst_type = row[\"dest_type_enc\"]\n",
    "                    \n",
    "                    if not isinstance(src, (int, np.integer)) or not isinstance(dst, (int, np.integer)):\n",
    "                        print_status(f\"graph_id {graph_id}，无效编码: src={src}, dst={dst}\")\n",
    "                        logger.error(f\"graph_id {graph_id}，无效编码: src={src}, dst={dst}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if src not in node_timestamps:\n",
    "                        node_timestamps[src] = len(node_timestamps)\n",
    "                        node_types[src] = src_type\n",
    "                        node_count += 1\n",
    "                    if dst not in node_timestamps:\n",
    "                        node_timestamps[dst] = len(node_timestamps)\n",
    "                        node_types[dst] = dst_type\n",
    "                        node_count += 1\n",
    "                    cache_graph[(src, dst)].append(edge_type)\n",
    "                    edge_batch_count += 1\n",
    "                    \n",
    "                    # 每500条边尝试生成快照\n",
    "                    if edge_batch_count >= 500:\n",
    "                        if generate_snapshot():\n",
    "                            sorted_nodes = sorted(node_timestamps.items(), key=lambda x: x[1])\n",
    "                            remove_count = int(n * fr)\n",
    "                            for node, _ in sorted_nodes[:remove_count]:\n",
    "                                del node_timestamps[node]\n",
    "                                del node_types[node]\n",
    "                                cache_graph.pop((node, None), None)\n",
    "                                cache_graph.pop((None, node), None)\n",
    "                            node_count = len(node_timestamps)\n",
    "                            gc.collect()\n",
    "                            print_status(f\"清理节点后，剩余节点: {node_count}, 内存: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "                            logger.info(f\"快照 {snapshot_id} 生成，剩余节点: {node_count}\")\n",
    "                        edge_batch_count = 0\n",
    "                \n",
    "                print_status(f\"处理graph_id {graph_id} 边耗时: {time.time() - edge_start:.2f}秒\")\n",
    "                logger.info(f\"处理graph_id {graph_id} 边耗时: {time.time() - edge_start:.2f}秒\")\n",
    "                \n",
    "                # graph_id 结束时强制生成快照\n",
    "                if cache_graph:\n",
    "                    generate_snapshot()\n",
    "                    sorted_nodes = sorted(node_timestamps.items(), key=lambda x: x[1])\n",
    "                    remove_count = int(n * fr)\n",
    "                    for node, _ in sorted_nodes[:remove_count]:\n",
    "                        del node_timestamps[node]\n",
    "                        del node_types[node]\n",
    "                        cache_graph.pop((node, None), None)\n",
    "                        cache_graph.pop((None, node), None)\n",
    "                    node_count = len(node_timestamps)\n",
    "                    gc.collect()\n",
    "                    print_status(f\"graph_id {graph_id} 清理节点后，剩余节点: {node_count}, 内存: {psutil.virtual_memory().used / 1024**3:.2f} GB\")\n",
    "                \n",
    "                print_status(f\"处理graph_id {graph_id} 完成，耗时: {time.time() - chunk_start:.2f}秒\")\n",
    "                logger.info(f\"处理graph_id {graph_id} 完成，耗时: {time.time() - chunk_start:.2f}秒\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print_status(f\"处理graph_id {graph_id} 失败: {e}\")\n",
    "                logger.error(f\"处理graph_id {graph_id} 失败: {e}\")\n",
    "        \n",
    "        # chunk 结束时强制生成快照\n",
    "        if cache_graph:\n",
    "            generate_snapshot()\n",
    "        \n",
    "        print_status(f\"处理chunk {chunk_idx}，耗时: {time.time() - chunk_start:.2f}秒\")\n",
    "        logger.info(f\"处理chunk {chunk_idx}，耗时: {time.time() - chunk_start:.2f}秒\")\n",
    "        gc.collect()\n",
    "    \n",
    "    if cache_graph:\n",
    "        generate_snapshot()\n",
    "    \n",
    "    print_status(f\"生成 {snapshot_id} 个快照，保存到 {output_dir}\")\n",
    "    print(f\"快照生成总耗时: {time.time() - start_snapshots:.2f}秒\")\n",
    "    print(f\"总运行时间: {time.time() - start_time:.2f}秒\")\n",
    "    logger.info(f\"生成 {snapshot_id} 个快照，总耗时: {time.time() - start_snapshots:.2f}秒\")\n",
    "    \n",
    "    # 验证日志和快照文件\n",
    "    for path in [log_path, fallback_log_path]:\n",
    "        if os.path.exists(path):\n",
    "            print_status(f\"日志文件生成: {path}\")\n",
    "            logger.info(f\"日志文件生成: {path}\")\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                print_status(f\"{path} 最后10行:\\n{''.join(f.readlines()[-10:])}\")\n",
    "        else:\n",
    "            print_status(f\"日志文件未生成: {path}\")\n",
    "            logger.error(f\"日志文件未生成: {path}\")\n",
    "    \n",
    "    pt_files = [f for f in os.listdir(output_dir) if f.endswith('.pt') and f.startswith('snapshot_')]\n",
    "    print_status(f\"生成 {len(pt_files)} 个快照文件: {pt_files[:5]}\")\n",
    "    logger.info(f\"生成 {len(pt_files)} 个快照文件: {pt_files[:5]}\")\n",
    "    \n",
    "    return snapshot_id\n",
    "\n",
    "# 执行快照生成\n",
    "print(\"执行快照生成...\")\n",
    "snapshot_count = generate_snapshots_chunks(DATA_PATH, node_encoder, node_type_encoder, edge_encoder)\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54018bab-a2b9-468f-967b-312fc3cb6eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试保存成功: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import os\n",
    "OUTPUT_DIR = r\".\\Data\\StreamSpot\\processed\"\n",
    "g = dgl.graph(([0, 1], [1, 2]))\n",
    "try:\n",
    "    torch.save(g, os.path.join(OUTPUT_DIR, \"test.pt\"))\n",
    "    print(f\"测试保存成功: {os.path.exists(os.path.join(OUTPUT_DIR, 'test.pt'))}\")\n",
    "except Exception as e:\n",
    "    print(f\"测试保存失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "785704c8-2946-4539-9fcf-b851addcea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主日志文件是否存在: False\n",
      "OUTPUT_DIR 绝对路径: C:\\Users\\cli305\\Codes\\Jupyter\\Provenance Graph Embedding\\Prographer on Provenance Graph Embedding\\Data\\StreamSpot\\processed\n",
      "当前工作目录: C:\\Users\\cli305\\Codes\\Jupyter\\Provenance Graph Embedding\\Prographer on Provenance Graph Embedding\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "OUTPUT_DIR = r\".\\Data\\StreamSpot\\processed\"\n",
    "log_path = os.path.abspath(os.path.join(OUTPUT_DIR, \"process.log\"))\n",
    "fallback_log_path = os.path.abspath(\"fallback_process.log\")\n",
    "try:\n",
    "    logging.basicConfig(filename=log_path, level=logging.INFO, format='%(asctime)s - %(message)s', filemode='w')\n",
    "    logger = logging.getLogger()\n",
    "    logger.info(\"测试日志写入\")\n",
    "    print(f\"主日志文件是否存在: {os.path.exists(log_path)}\")\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            print(f\"主日志内容:\\n{f.read()}\")\n",
    "except Exception as e:\n",
    "    print(f\"主日志配置失败: {e}\")\n",
    "    try:\n",
    "        logging.basicConfig(filename=fallback_log_path, level=logging.INFO, format='%(asctime)s - %(message)s', filemode='w')\n",
    "        logger = logging.getLogger()\n",
    "        logger.info(\"测试备用日志写入\")\n",
    "        print(f\"备用日志文件是否存在: {os.path.exists(fallback_log_path)}\")\n",
    "        if os.path.exists(fallback_log_path):\n",
    "            with open(fallback_log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                print(f\"备用日志内容:\\n{f.read()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"备用日志配置失败: {e}\")\n",
    "print(f\"OUTPUT_DIR 绝对路径: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(f\"当前工作目录: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c4bf13f-fec2-4138-9958-5ca7a474aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个chunk的graph_id: [2, 0, 3, 1, 4]\n",
      "graph_id 0-9 边数: ['graph_id 0: 104968', 'graph_id 1: 76512', 'graph_id 2: 210845', 'graph_id 3: 79365', 'graph_id 4: 28310', 'graph_id 5: 0', 'graph_id 6: 0', 'graph_id 7: 0', 'graph_id 8: 0', 'graph_id 9: 0']\n",
      "有效graph_id (边数 <= 20000): []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH = r\".\\Data\\StreamSpot\\all.tsv\"\n",
    "chunk_size = 500000\n",
    "chunk = next(pd.read_csv(DATA_PATH, sep='\\t', header=None,\n",
    "                         names=[\"source_id\", \"source_type\", \"dest_id\", \n",
    "                                \"dest_type\", \"edge_type\", \"graph_id\"],\n",
    "                         dtype={\"source_id\": str, \"dest_id\": str, \"graph_id\": int},\n",
    "                         chunksize=chunk_size))\n",
    "graph_id_counts = chunk[\"graph_id\"].value_counts()\n",
    "valid_graph_ids = [gid for gid in graph_id_counts.index if 0 <= gid <= 9]\n",
    "print(f\"第一个chunk的graph_id: {list(graph_id_counts.index)}\")\n",
    "print(f\"graph_id 0-9 边数: {[f'graph_id {gid}: {graph_id_counts.get(gid, 0)}' for gid in range(10)]}\")\n",
    "print(f\"有效graph_id (边数 <= 20000): {[gid for gid in valid_graph_ids if graph_id_counts[gid] <= 20000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f119f29-366f-4427-8919-5a20f91f8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_id 4, 边数: 28310, cache_graph大小: 557, 节点数: 558, 边批次: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "DATA_PATH = r\".\\Data\\StreamSpot\\all.tsv\"\n",
    "chunk_size = 500000\n",
    "chunk = next(pd.read_csv(DATA_PATH, sep='\\t', header=None,\n",
    "                         names=[\"source_id\", \"source_type\", \"dest_id\", \n",
    "                                \"dest_type\", \"edge_type\", \"graph_id\"],\n",
    "                         dtype={\"source_id\": str, \"dest_id\": str, \"graph_id\": int},\n",
    "                         chunksize=chunk_size))\n",
    "chunk[\"source_id_enc\"] = node_encoder.transform(chunk[\"source_id\"].astype(str))\n",
    "chunk[\"dest_id_enc\"] = node_encoder.transform(chunk[\"dest_id\"].astype(str))\n",
    "chunk[\"source_type_enc\"] = node_type_encoder.transform(chunk[\"source_type\"])\n",
    "chunk[\"dest_type_enc\"] = node_type_encoder.transform(chunk[\"dest_type\"])\n",
    "chunk[\"edge_type_enc\"] = edge_encoder.transform(chunk[\"edge_type\"])\n",
    "graph_id_counts = chunk[\"graph_id\"].value_counts()\n",
    "valid_graph_ids = [gid for gid in graph_id_counts.index if 0 <= gid <= 9 and graph_id_counts[gid] <= 50000]\n",
    "if not valid_graph_ids:\n",
    "    print(\"无有效graph_id\")\n",
    "else:\n",
    "    graph_id = valid_graph_ids[0]\n",
    "    sub_df = chunk[chunk[\"graph_id\"] == graph_id]\n",
    "    cache_graph = defaultdict(list)\n",
    "    node_timestamps = {}\n",
    "    node_types = {}\n",
    "    node_count = 0\n",
    "    edge_batch_count = 0\n",
    "    for idx in range(min(1000, len(sub_df))):\n",
    "        row = sub_df.iloc[idx]\n",
    "        src = row[\"source_id_enc\"]\n",
    "        dst = row[\"dest_id_enc\"]\n",
    "        edge_type = row[\"edge_type_enc\"]\n",
    "        src_type = row[\"source_type_enc\"]\n",
    "        dst_type = row[\"dest_type_enc\"]\n",
    "        if not isinstance(src, (int, np.integer)) or not isinstance(dst, (int, np.integer)):\n",
    "            print(f\"无效编码: src={src}, dst={dst}\")\n",
    "            continue\n",
    "        if src not in node_timestamps:\n",
    "            node_timestamps[src] = len(node_timestamps)\n",
    "            node_types[src] = src_type\n",
    "            node_count += 1\n",
    "        if dst not in node_timestamps:\n",
    "            node_timestamps[dst] = len(node_timestamps)\n",
    "            node_types[dst] = dst_type\n",
    "            node_count += 1\n",
    "        cache_graph[(src, dst)].append(edge_type)\n",
    "        edge_batch_count += 1\n",
    "    print(f\"graph_id {graph_id}, 边数: {len(sub_df)}, cache_graph大小: {len(cache_graph)}, 节点数: {node_count}, 边批次: {edge_batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed1ff48b-34b7-4cf5-bab1-d0d40510d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGL图生成成功: 节点数=501, 边数=500\n",
      "快照保存成功: True\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "OUTPUT_DIR = r\".\\Data\\StreamSpot\\processed\"\n",
    "cache_graph = defaultdict(list)\n",
    "node_types = {}\n",
    "for i in range(500):\n",
    "    src, dst = i, i + 1\n",
    "    cache_graph[(src, dst)].append(0)\n",
    "    node_types[src] = 0\n",
    "    node_types[dst] = 0\n",
    "src_ids, dst_ids, edge_types = [], [], []\n",
    "for (s, d), etypes in cache_graph.items():\n",
    "    src_ids.append(s)\n",
    "    dst_ids.append(d)\n",
    "    edge_types.append(max(etypes))\n",
    "try:\n",
    "    g = dgl.graph((torch.tensor(src_ids, dtype=torch.int64), torch.tensor(dst_ids, dtype=torch.int64)))\n",
    "    g.edata[\"type\"] = torch.tensor(edge_types, dtype=torch.int64)\n",
    "    node_type_list = [node_types.get(i, 0) for i in range(g.num_nodes())]\n",
    "    g.ndata[\"type\"] = torch.tensor(node_type_list, dtype=torch.int64)\n",
    "    g.ndata[\"id\"] = torch.arange(g.num_nodes(), dtype=torch.int64)\n",
    "    print(f\"DGL图生成成功: 节点数={g.num_nodes()}, 边数={g.num_edges()}\")\n",
    "    snapshot_path = os.path.join(OUTPUT_DIR, \"snapshot_test.pt\")\n",
    "    torch.save(g, snapshot_path)\n",
    "    print(f\"快照保存成功: {os.path.exists(snapshot_path)}\")\n",
    "except Exception as e:\n",
    "    print(f\"快照生成或保存失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae952263-c8ee-483f-a501-c5672553e8be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
